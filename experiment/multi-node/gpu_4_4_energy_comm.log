compilation for architecture [gpu] using compilation chain [nvidia]

-- The CXX compiler identification is NVHPC 24.7.0
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/compilers/bin/nvc++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found NetCDF: /sw/spack-levante/netcdf-c-main-k4lh4v/include (found version "4.9.3") found components: C CXX 
-- FindNetCDF defines targets:
--   - NetCDF::NetCDF_C [/sw/spack-levante/netcdf-c-main-k4lh4v/lib64/libnetcdf.so]
--   - NetCDF::NetCDF_CXX [/sw/spack-levante/netcdf-cxx4-4.3.1-42ju4n/lib/libnetcdf_c++4.so]
-- Found MPI_CXX: /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/libmpi.so (found version "3.1") 
-- Found MPI: TRUE (found version "3.1")  
-- The C compiler identification is NVHPC 24.7.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/compilers/bin/nvc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Build Googletest as part of muphys project
-- Found Python: /sw/spack-levante/python-3.10.10-uigoar/bin/python3.10 (found version "3.10.10") found components: Interpreter 
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Configuring done
-- Generating done
-- Build files have been written to: /home/b/b383366/sky/scc_at_isc25/build_std_gpu_mpi
[  5%] Building CXX object core/CMakeFiles/muphys_core.dir/common/utils.cpp.o
[ 16%] Building CXX object io/CMakeFiles/muphys_io.dir/io.cpp.o
[ 16%] Building CXX object _deps/googletest-build/googletest/CMakeFiles/gtest.dir/src/gtest-all.cc.o
[ 22%] Building CXX object implementations/std/CMakeFiles/muphys_implementation.dir/graupel.cpp.o
"/home/b/b383366/sky/scc_at_isc25/build_std_gpu_mpi/_deps/googletest-src/googletest/src/gtest-internal-inl.h", line 636: warning: unknown attribute "optimize" [unrecognized_attribute]
        GTEST_NO_INLINE_ GTEST_NO_TAIL_CALL_;
                         ^

Remark: individual warnings can be suppressed with "--diag_suppress <warning-name>"

[ 27%] Linking CXX shared library ../lib64/libmuphys_core.so
"/home/b/b383366/sky/scc_at_isc25/build_std_gpu_mpi/_deps/googletest-src/googletest/src/gtest.cc", line 6262: warning: unknown attribute "optimize" [unrecognized_attribute]
  GTEST_NO_INLINE_ GTEST_NO_TAIL_CALL_ std::string
                   ^

[ 27%] Built target muphys_core
[ 33%] Linking CXX shared library ../lib64/libmuphys_io.so
[ 33%] Built target muphys_io
[ 38%] Linking CXX shared library ../../lib64/libmuphys_implementation.so
[ 38%] Built target muphys_implementation
[ 44%] Building CXX object CMakeFiles/graupel.dir/main_mpi.cpp.o
[ 50%] Linking CXX static library ../../../lib/libgtest.a
[ 55%] Linking CXX executable bin/graupel
[ 55%] Built target gtest
[ 61%] Building CXX object _deps/googletest-build/googlemock/CMakeFiles/gmock.dir/src/gmock-all.cc.o
[ 66%] Building CXX object _deps/googletest-build/googletest/CMakeFiles/gtest_main.dir/src/gtest_main.cc.o
[ 66%] Built target graupel
[ 72%] Linking CXX static library ../../../lib/libgtest_main.a
[ 72%] Built target gtest_main
[ 77%] Building CXX object test/CMakeFiles/muphys_core_test.dir/common.cc.o
[ 83%] Linking CXX static library ../../../lib/libgmock.a
[ 83%] Built target gmock
[ 88%] Building CXX object _deps/googletest-build/googlemock/CMakeFiles/gmock_main.dir/src/gmock_main.cc.o
[ 94%] Linking CXX executable ../bin/muphys_core_test
[ 94%] Built target muphys_core_test
[100%] Linking CXX static library ../../../lib/libgmock_main.a
[100%] Built target gmock_main
 2: Compute process 2 on l50057.lvt.dkrz.de
 3: Compute process 3 on l50057.lvt.dkrz.de
 0: Compute process 0 on l50057.lvt.dkrz.de
 1: Compute process 1 on l50057.lvt.dkrz.de
 0: + numactl --cpunodebind=2-3 --membind=2-3 build_std_gpu_mpi/bin/graupel /work/ka1273/atm_R2B08.nc /scratch/b/b383366/outputs/output.nc
 4: Compute process 0 on l50066.lvt.dkrz.de
 7: Compute process 3 on l50066.lvt.dkrz.de
 5: Compute process 1 on l50066.lvt.dkrz.de
 6: Compute process 2 on l50066.lvt.dkrz.de
 4: + numactl --cpunodebind=2-3 --membind=2-3 build_std_gpu_mpi/bin/graupel /work/ka1273/atm_R2B08.nc /scratch/b/b383366/outputs/output.nc
10: Compute process 2 on l50069.lvt.dkrz.de
11: Compute process 3 on l50069.lvt.dkrz.de
 8: Compute process 0 on l50069.lvt.dkrz.de
 9: Compute process 1 on l50069.lvt.dkrz.de
 8: + numactl --cpunodebind=2-3 --membind=2-3 build_std_gpu_mpi/bin/graupel /work/ka1273/atm_R2B08.nc /scratch/b/b383366/outputs/output.nc
12: Compute process 0 on l50075.lvt.dkrz.de
13: Compute process 1 on l50075.lvt.dkrz.de
15: Compute process 3 on l50075.lvt.dkrz.de
14: Compute process 2 on l50075.lvt.dkrz.de
12: + numactl --cpunodebind=2-3 --membind=2-3 build_std_gpu_mpi/bin/graupel /work/ka1273/atm_R2B08.nc /scratch/b/b383366/outputs/output.nc
 0: [1746369121.715405] [l50057:596289:0]     ucp_context.c:1086 UCX  WARN  transport 'gdr_copy' is not available, please use one or more of: cma, cuda, cuda_copy, cuda_ipc, dc, dc_mlx5, dc_x, ib, mm, posix, rc, rc_mlx5, rc_v, rc_verbs, rc_x, self, shm, sm, sysv, tcp, ud, ud_mlx5, ud_v, ud_verbs, ud_x, xpmem
 0: [1746369123.127858] [l50057:596289:0]     ucp_context.c:1086 UCX  WARN  transport 'gdr_copy' is not available, please use one or more of: cma, cuda, cuda_copy, cuda_ipc, dc, dc_mlx5, dc_x, ib, mm, posix, rc, rc_mlx5, rc_v, rc_verbs, rc_x, self, shm, sm, sysv, tcp, ud, ud_mlx5, ud_v, ud_verbs, ud_x, xpmem
 0: multirun =1
12: [1746369122.972131] [l50075:3131922:0]     ucp_context.c:1086 UCX  WARN  transport 'gdr_copy' is not available, please use one or more of: cma, cuda, cuda_copy, cuda_ipc, dc, dc_mlx5, dc_x, ib, mm, posix, rc, rc_mlx5, rc_v, rc_verbs, rc_x, self, shm, sm, sysv, tcp, ud, ud_mlx5, ud_v, ud_verbs, ud_x, xpmem
12: [1746369123.138000] [l50075:3131922:0]     ucp_context.c:1086 UCX  WARN  transport 'gdr_copy' is not available, please use one or more of: cma, cuda, cuda_copy, cuda_ipc, dc, dc_mlx5, dc_x, ib, mm, posix, rc, rc_mlx5, rc_v, rc_verbs, rc_x, self, shm, sm, sysv, tcp, ud, ud_mlx5, ud_v, ud_verbs, ud_x, xpmem
14: [1746369122.482507] [l50075:3131920:0]     ucp_context.c:1086 UCX  WARN  transport 'gdr_copy' is not available, please use one or more of: cma, cuda, cuda_copy, cuda_ipc, dc, dc_mlx5, dc_x, ib, mm, posix, rc, rc_mlx5, rc_v, rc_verbs, rc_x, self, shm, sm, sysv, tcp, ud, ud_mlx5, ud_v, ud_verbs, ud_x, xpmem
14: [1746369123.127173] [l50075:3131920:0]     ucp_context.c:1086 UCX  WARN  transport 'gdr_copy' is not available, please use one or more of: cma, cuda, cuda_copy, cuda_ipc, dc, dc_mlx5, dc_x, ib, mm, posix, rc, rc_mlx5, rc_v, rc_verbs, rc_x, self, shm, sm, sysv, tcp, ud, ud_mlx5, ud_v, ud_verbs, ud_x, xpmem
15: [1746369122.727030] [l50075:3131919:0]     ucp_context.c:1086 UCX  WARN  transport 'gdr_copy' is not available, please use one or more of: cma, cuda, cuda_copy, cuda_ipc, dc, dc_mlx5, dc_x, ib, mm, posix, rc, rc_mlx5, rc_v, rc_verbs, rc_x, self, shm, sm, sysv, tcp, ud, ud_mlx5, ud_v, ud_verbs, ud_x, xpmem
15: [1746369123.127134] [l50075:3131919:0]     ucp_context.c:1086 UCX  WARN  transport 'gdr_copy' is not available, please use one or more of: cma, cuda, cuda_copy, cuda_ipc, dc, dc_mlx5, dc_x, ib, mm, posix, rc, rc_mlx5, rc_v, rc_verbs, rc_x, self, shm, sm, sysv, tcp, ud, ud_mlx5, ud_v, ud_verbs, ud_x, xpmem
13: [1746369122.303899] [l50075:3131921:0]     ucp_context.c:1086 UCX  WARN  transport 'gdr_copy' is not available, please use one or more of: cma, cuda, cuda_copy, cuda_ipc, dc, dc_mlx5, dc_x, ib, mm, posix, rc, rc_mlx5, rc_v, rc_verbs, rc_x, self, shm, sm, sysv, tcp, ud, ud_mlx5, ud_v, ud_verbs, ud_x, xpmem
13: [1746369123.127100] [l50075:3131921:0]     ucp_context.c:1086 UCX  WARN  transport 'gdr_copy' is not available, please use one or more of: cma, cuda, cuda_copy, cuda_ipc, dc, dc_mlx5, dc_x, ib, mm, posix, rc, rc_mlx5, rc_v, rc_verbs, rc_x, self, shm, sm, sysv, tcp, ud, ud_mlx5, ud_v, ud_verbs, ud_x, xpmem
12: + kill_nvsmi
12: + set +x
 0: [l50057:596289:0:596289] ib_mlx5_log.c:177  Transport retry count exceeded on mlx5_0:1/IB (synd 0x15 vend 0x81 hw_synd 0/0)
 0: [l50057:596289:0:596289] ib_mlx5_log.c:177  RC QP 0xdf0 wqe[55383]: SEND --- [inl len 2] [rqpn 0x11b18 dlid=3327 sl=0 port=1 src_path_bits=0]
 0: ==== backtrace (tid: 596289) ====
 0:  0  /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/libucs.so.0(ucs_handle_error+0x124) [0x15554eccc364]
 0:  1  /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/libucs.so.0(ucs_fatal_error_message+0x50) [0x15554ecc9690]
 0:  2  /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/libucs.so.0(+0x33a27) [0x15554eccda27]
 0:  3  /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/libucs.so.0(ucs_log_dispatch+0xdf) [0x15554eccddef]
 0:  4  /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/ucx/libuct_ib.so.0(uct_ib_mlx5_completion_with_err+0x728) [0x155548de6f78]
 0:  5  /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/ucx/libuct_ib.so.0(+0x4dc97) [0x155548e0bc97]
 0:  6  /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/ucx/libuct_ib.so.0(uct_ib_mlx5_check_completion_with_err+0x6f) [0x155548de7f8f]
 0:  7  /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/ucx/libuct_ib.so.0(uct_ib_mlx5_check_completion+0x34) [0x155548de7fd4]
 0:  8  /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/ucx/libuct_ib.so.0(+0x4f92b) [0x155548e0d92b]
 0:  9  /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x15554f1b44ba]
 0: 10  /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/libmpi.so.40(mca_pml_ucx_progress+0x14) [0x15555472d454]
 0: 11  /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/libopen-pal.so.40(opal_progress+0x2d) [0x15554f4fdaad]
 0: 12  /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/libmpi.so.40(ompi_request_default_wait_all+0x215) [0x1555545711d5]
 0: 13  /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/libmpi.so.40(PMPI_Waitall+0x8c) [0x1555545b0e0c]
 0: 14  build_std_gpu_mpi/bin/graupel() [0x4052f4]
 0: 15  /lib64/libc.so.6(__libc_start_main+0xe5) [0x1555502edd85]
 0: 16  build_std_gpu_mpi/bin/graupel() [0x40309e]
 0: =================================
 0: [l50057:596289] *** Process received signal ***
 0: [l50057:596289] Signal: Aborted (6)
 0: [l50057:596289] Signal code:  (-6)
 0: [l50057:596289] [ 0] /lib64/libpthread.so.0(+0x12cf0)[0x155550f64cf0]
 0: [l50057:596289] [ 1] /lib64/libc.so.6(gsignal+0x10f)[0x155550301acf]
 0: [l50057:596289] [ 2] /lib64/libc.so.6(abort+0x127)[0x1555502d4ea5]
 0: [l50057:596289] [ 3] /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/libucs.so.0(ucs_fatal_error_message+0x55)[0x15554ecc9695]
 0: [l50057:596289] [ 4] /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/libucs.so.0(+0x33a27)[0x15554eccda27]
 0: [l50057:596289] [ 5] /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/libucs.so.0(ucs_log_dispatch+0xdf)[0x15554eccddef]
 0: [l50057:596289] [ 6] /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/ucx/libuct_ib.so.0(uct_ib_mlx5_completion_with_err+0x728)[0x155548de6f78]
 0: [l50057:596289] [ 7] /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/ucx/libuct_ib.so.0(+0x4dc97)[0x155548e0bc97]
 0: [l50057:596289] [ 8] /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/ucx/libuct_ib.so.0(uct_ib_mlx5_check_completion_with_err+0x6f)[0x155548de7f8f]
 0: [l50057:596289] [ 9] /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/ucx/libuct_ib.so.0(uct_ib_mlx5_check_completion+0x34)[0x155548de7fd4]
 0: [l50057:596289] [10] /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/ucx/libuct_ib.so.0(+0x4f92b)[0x155548e0d92b]
 0: [l50057:596289] [11] /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/libucp.so.0(ucp_worker_progress+0x6a)[0x15554f1b44ba]
 0: [l50057:596289] [12] /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/libmpi.so.40(mca_pml_ucx_progress+0x14)[0x15555472d454]
 0: [l50057:596289] [13] /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/libopen-pal.so.40(opal_progress+0x2d)[0x15554f4fdaad]
 0: [l50057:596289] [14] /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/libmpi.so.40(ompi_request_default_wait_all+0x215)[0x1555545711d5]
 0: [l50057:596289] [15] /sw/spack-levante/nvhpc-24.7-py26uc/Linux_x86_64/24.7/comm_libs/12.5/openmpi4/openmpi-4.1.5/lib/libmpi.so.40(PMPI_Waitall+0x8c)[0x1555545b0e0c]
 0: [l50057:596289] [16] build_std_gpu_mpi/bin/graupel[0x4052f4]
 0: [l50057:596289] [17] /lib64/libc.so.6(__libc_start_main+0xe5)[0x1555502edd85]
 0: [l50057:596289] [18] build_std_gpu_mpi/bin/graupel[0x40309e]
 0: [l50057:596289] *** End of error message ***
 0: /home/b/b383366/sky/scc_at_isc25/scripts/run_wrapper_levante.sh: line 155: 596289 Aborted                 numactl --cpunodebind=${numanode_reorder[$lrank]} --membind=${numanode_reorder[$lrank]} $executable $input_file $output_file
 0: ++ kill_nvsmi
 0: ++ set +x
srun: error: l50057: task 0: Exited with exit code 134
srun: Terminating StepId=16752639.0
 0: slurmstepd: error: *** STEP 16752639.0 ON l50057 CANCELLED AT 2025-05-04T16:43:00 ***
 0: slurmstepd: error:  mpi/pmix_v3: _errhandler: l50057 [0]: pmixp_client_v2.c:212: Error handler invoked: status = -25, source = [slurm.pmix.16752639.0:0]
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
 0: slurmstepd: error:  mpi/pmix_v3: _errhandler: l50057 [0]: pmixp_client_v2.c:212: Error handler invoked: status = -25, source = [slurm.pmix.16752639.0:1]
 8: slurmstepd: error:  mpi/pmix_v3: _errhandler: l50069 [2]: pmixp_client_v2.c:212: Error handler invoked: status = -25, source = [slurm.pmix.16752639.0:9]
 4: slurmstepd: error:  mpi/pmix_v3: _errhandler: l50066 [1]: pmixp_client_v2.c:212: Error handler invoked: status = -25, source = [slurm.pmix.16752639.0:4]
srun: error: l50057: tasks 1-3: Terminated
srun: error: l50066: tasks 4-7: Terminated
srun: error: l50069: tasks 8-11: Terminated
               Date     Time   Level Gridsize    Miss    Diff : S Z  Max_Absdiff Max_Reldiff : Parameter name
     1 : 0000-00-00 00:00:00       0 471859200       0    1201 : F F   1.7053e-13  5.7125e-16 : ta         
     2 : 0000-00-00 00:00:00       0 471859200       0    4138 : F F   3.4694e-18  5.4268e-16 : hus        
     3 : 0000-00-00 00:00:00       0 471859200       0    3121 : F T   8.6736e-19     0.66667 : clw        
     4 : 0000-00-00 00:00:00       0 471859200       0  487315 : F T   2.1684e-19      1.0000 : cli        
     5 : 0000-00-00 00:00:00       0 471859200       0  440152 : F T   4.1200e-18     0.97472 : qr         
     6 : 0000-00-00 00:00:00       0 471859200       0  550316 : F T   1.3010e-18  0.00014277 : qs         
     7 : 0000-00-00 00:00:00       0 471859200       0  379166 : F T   1.7347e-18  3.0237e-05 : qg         
     8 : 0000-00-00 00:00:00       0 471859200       0 1671692 : F T   1.3878e-17     0.99998 : pflx       
     9 : 0000-00-00 00:00:00       0  5242880       0   54773 : F T   3.4694e-18     0.29444 : prr_gsp    
    10 : 0000-00-00 00:00:00       0  5242880       0   42605 : F T   4.3368e-19  1.7510e-09 : prs_gsp    
    11 : 0000-00-00 00:00:00       0  5242880       0   39630 : F T   3.3881e-21     0.23588 : pri_gsp    
    12 : 0000-00-00 00:00:00       0  5242880       0   23828 : F T   4.3368e-19  3.5586e-11 : prg_gsp    
    13 : 0000-00-00 00:00:00       0  5242880       0   63614 : F T   9.0949e-12  1.9204e-08 : pre_gsp    
  13 of 13 fields differ
  0 of 13 fields differ more than 0.001
slurmstepd: error: *** JOB 16752639 ON l50057 CANCELLED AT 2025-05-04T16:44:54 ***

********************************************************************************
*                                                                              *
*  This is the automated job summary provided by DKRZ.                         *
*  If you encounter problems, need assistance or have any suggestion, please   *
*  write an email to                                                           *
*                                                                              *
*  --  support@dkrz.de --                                                      *
*                                                                              *
*                       We hope you enjoyed the DKRZ supercomputer LEVANTE ... *
*
* JobID            : 16752639
* JobName          : scc                                               
* Account          : ka1273_gpu
* User             : b383366 (202646), ka1273 (201200)                 
* Partition        : gpu
* QOS              : normal
* Nodelist         : l[50057,50066,50069,50075] (4)                            
* Submit date      : 2025-05-04T16:30:46
* Start time       : 2025-05-04T16:30:47
* End time         : 2025-05-04T16:44:54
* Elapsed time     : 00:14:07 (Timelimit=00:30:00)                     
* Command          : ./scripts/levante-gpu-energy.batch
* WorkDir          : /home/b/b383366/sky/scc_at_isc25
*
* StepID | JobName      NodeHours    MaxRSS [Byte] (@task)
* ------------------------------------------------------------------------------
* 0      | run_wrapper_      0.74            51135452K (4)
* batch  | batch             0.94
* extern | extern            0.94                2160K (2)
* ------------------------------------------------------------------------------

